# -*- coding: utf-8 -*-
"""logic_neural_net.-0Bxi9Z_HdoQ6EQm0wbllXT0h5cVc1Q0FSWGJhRTA1TG1LUUJFPQ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cmVWAl9bbA6j51cmIhm0Dmy-rgwnBrdj
"""

import tensorflow as tf

# y = x/2 + 1
X = tf.constant([#1#2#3
                [0,0,0],
                [0,0,1],
                [0,1,0],
                [0,1,1],
                [1,0,0],
                [1,0,1],
                [1,1,0],
                [1,1,1],],tf.float32,shape=[3,8])

y = tf.constant([[0],[1],[0],[1],[0],[1],[0],[1]],tf.float32,shape=[1,8])

W1= tf.Variable(tf.random_normal([4, 3]))
W2= tf.Variable(tf.random_normal([1, 4])) 

b1 = tf.Variable(tf.random_normal([1,1]))
b2 = tf.Variable(tf.random_normal([1,1]))

L1 = tf.sigmoid(tf.add(tf.matmul(W1,X),b1))
L2 = tf.sigmoid(tf.add(tf.matmul(W2,L1),b2))

err = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=L2, labels=y))
#err = tf.reduce_mean(tf.subtract(y,L2)**2)
train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(err)

init_op = tf.global_variables_initializer()
sess = tf.Session()

sess.run(init_op)

for i in range(1,10000):
  sess.run(train)
  if(i%100 ==0):
    print(sess.run(err))
  
print(sess.run(err))

